# data-pipeline-with-spark
Using distributed processing framework of Apache Spark to create data lake and data lakehouse on Databricks platform

Databricks exports .ipynb notebook as .py which can be reverse when databricks notebooks are downloaded by running:
convert_databricks_nb('example_databricks_notebook.py', 'example_databricks_notebook_conv.ipynb')
And from an ipytthon nootebook to a .py file:
convert_databricks_nb('databricks_nb.ipynb','databricks_nb.py')

See [ipynb-py-convert-databricks](https://github.com/Yoyodyne-Data-Science/ipynb-py-convert-databricks/tree/master)



